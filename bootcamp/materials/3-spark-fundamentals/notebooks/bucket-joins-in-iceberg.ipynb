{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374411c5-a48f-4739-9031-d638638633a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://dd03eda02c1f:4042\n",
       "SparkContext available as 'sc' (version = 3.5.1, master = local[*], app id = local-1737518978446)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "org.apache.iceberg.exceptions.ServiceFailureException",
     "evalue": " Server error: NotFoundException: Location does not exist: s3://warehouse/bootcamp/matches_bucketed/metadata/00000-202d2d8d-7f15-4131-a499-66be82ed1977.metadata.json",
     "output_type": "error",
     "traceback": [
      "org.apache.iceberg.exceptions.ServiceFailureException: Server error: NotFoundException: Location does not exist: s3://warehouse/bootcamp/matches_bucketed/metadata/00000-202d2d8d-7f15-4131-a499-66be82ed1977.metadata.json",
      "  at org.apache.iceberg.rest.ErrorHandlers$DefaultErrorHandler.accept(ErrorHandlers.java:217)",
      "  at org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:118)",
      "  at org.apache.iceberg.rest.ErrorHandlers$TableErrorHandler.accept(ErrorHandlers.java:102)",
      "  at org.apache.iceberg.rest.HTTPClient.throwFailure(HTTPClient.java:201)",
      "  at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:313)",
      "  at org.apache.iceberg.rest.HTTPClient.execute(HTTPClient.java:252)",
      "  at org.apache.iceberg.rest.HTTPClient.get(HTTPClient.java:348)",
      "  at org.apache.iceberg.rest.RESTClient.get(RESTClient.java:96)",
      "  at org.apache.iceberg.rest.RESTSessionCatalog.loadInternal(RESTSessionCatalog.java:331)",
      "  at org.apache.iceberg.rest.RESTSessionCatalog.loadTable(RESTSessionCatalog.java:347)",
      "  at org.apache.iceberg.catalog.BaseSessionCatalog$AsCatalog.loadTable(BaseSessionCatalog.java:99)",
      "  at org.apache.iceberg.rest.RESTCatalog.loadTable(RESTCatalog.java:102)",
      "  at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.lambda$doComputeIfAbsent$14(BoundedLocalCache.java:2406)",
      "  at java.base/java.util.concurrent.ConcurrentHashMap.compute(ConcurrentHashMap.java:1908)",
      "  at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.doComputeIfAbsent(BoundedLocalCache.java:2404)",
      "  at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.BoundedLocalCache.computeIfAbsent(BoundedLocalCache.java:2387)",
      "  at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalCache.computeIfAbsent(LocalCache.java:108)",
      "  at org.apache.iceberg.shaded.com.github.benmanes.caffeine.cache.LocalManualCache.get(LocalManualCache.java:62)",
      "  at org.apache.iceberg.CachingCatalog.loadTable(CachingCatalog.java:166)",
      "  at org.apache.iceberg.spark.SparkCatalog.load(SparkCatalog.java:843)",
      "  at org.apache.iceberg.spark.SparkCatalog.loadTable(SparkCatalog.java:170)",
      "  at org.apache.spark.sql.connector.catalog.TableCatalog.tableExists(TableCatalog.java:164)",
      "  at org.apache.spark.sql.execution.datasources.v2.DropTableExec.run(DropTableExec.scala:36)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)",
      "  at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)",
      "  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)",
      "  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)",
      "  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)",
      "  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)",
      "  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)",
      "  at org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:691)",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:682)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:713)",
      "  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:744)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "// In python use: from pyspark.sql.functions import broadcast, split, lit\n",
    "import org.apache.spark.sql.functions.{broadcast, split, lit}\n",
    "\n",
    "\n",
    "val matchesBucketed = spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/matches.csv\")\n",
    "val matchDetailsBucketed =  spark.read.option(\"header\", \"true\")\n",
    "                        .option(\"inferSchema\", \"true\")\n",
    "                        .csv(\"/home/iceberg/data/match_details.csv\")\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"DROP TABLE IF EXISTS bootcamp.matches_bucketed\"\"\")\n",
    "val bucketedDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.matches_bucketed (\n",
    "     match_id STRING,\n",
    "     is_team_game BOOLEAN,\n",
    "     playlist_id STRING,\n",
    "     completion_date TIMESTAMP\n",
    " )\n",
    " USING iceberg\n",
    " PARTITIONED BY (completion_date, bucket(16, match_id));\n",
    "\"\"\"\n",
    "spark.sql(bucketedDDL)\n",
    "\n",
    "matchesBucketed.select(\n",
    "    $\"match_id\", $\"is_team_game\", $\"playlist_id\", $\"completion_date\"\n",
    "    )\n",
    "    .write.mode(\"append\")\n",
    "    .partitionBy(\"completion_date\")\n",
    "  .bucketBy(16, \"match_id\").saveAsTable(\"bootcamp.matches_bucketed\")\n",
    "\n",
    "\n",
    "val bucketedDetailsDDL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS bootcamp.match_details_bucketed (\n",
    "    match_id STRING,\n",
    "    player_gamertag STRING,\n",
    "    player_total_kills INTEGER,\n",
    "    player_total_deaths INTEGER\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (bucket(16, match_id));\n",
    "\"\"\"\n",
    "spark.sql(bucketedDetailsDDL)\n",
    "\n",
    "matchDetailsBucketed.select(\n",
    "    $\"match_id\", $\"player_gamertag\", $\"player_total_kills\", $\"player_total_deaths\")\n",
    "    .write.mode(\"append\")\n",
    "    .bucketBy(16, \"match_id\").saveAsTable(\"bootcamp.match_details_bucketed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b964696-1e04-4c4f-a0d9-db70e266b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "// Disable broadcast join to force the bucket join.\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "matchesBucketed.createOrReplaceTempView(\"matches\")\n",
    "matchDetailsBucketed.createOrReplaceTempView(\"match_details\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM bootcamp.match_details_bucketed mdb JOIN bootcamp.matches_bucketed md \n",
    "    ON mdb.match_id = md.match_id\n",
    "    AND md.completion_date = DATE('2016-01-01')\n",
    "        \n",
    "\"\"\").explain()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM match_details mdb JOIN matches md ON mdb.match_id = md.match_id\n",
    "        \n",
    "\"\"\").explain()\n",
    "\n",
    "// spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"1000000000000\")\n",
    "\n",
    "// val broadcastFromThreshold = matches.as(\"m\").join(matchDetails.as(\"md\"), $\"m.match_id\" === $\"md.match_id\")\n",
    "//   .select($\"m.completion_date\", $\"md.player_gamertag\",  $\"md.player_total_kills\")\n",
    "//   .take(5)\n",
    "\n",
    "// val explicitBroadcast = matches.as(\"m\").join(broadcast(matchDetails).as(\"md\"), $\"m.match_id\" === $\"md.match_id\")\n",
    "//   .select($\"md.*\", split($\"completion_date\", \" \").getItem(0).as(\"ds\"))\n",
    "\n",
    "// val bucketedValues = matchDetailsBucketed.as(\"mdb\").join(matchesBucketed.as(\"mb\"), $\"mb.match_id\" === $\"mdb.match_id\").explain()\n",
    "// // .take(5)\n",
    "\n",
    "// val values = matchDetailsBucketed.as(\"m\").join(matchesBucketed.as(\"md\"), $\"m.match_id\" === $\"md.match_id\").explain()\n",
    "\n",
    "// explicitBroadcast.write.mode(\"overwrite\").insertInto(\"match_details_bucketed\")\n",
    "\n",
    "// matches.withColumn(\"ds\", split($\"completion_date\", \" \").getItem(0)).write.mode(\"overwrite\").insertInto(\"matches_bucketed\")\n",
    "\n",
    "// spark.sql(bucketedSQL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8adb02-d5bd-4e84-a671-48991772d233",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1eecb6-ca9a-4b5c-b046-b3a0dd1ff3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
